{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a tool provided by another developer as an interface to download all of the issues from the React.js GitHub repo. The tool was downloaded from the following repository:\n",
    "\n",
    "https://github.com/gavinr/github-csv-tools\n",
    "\n",
    "The output from this tool was a csv file with all issues from the React.js repository, and the code below processes the data to provide us with our 1000 sample dataset for the following labelling and Machine Learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 14:39:37 WARN Utils: Your hostname, graeme-IdeaPad resolves to a loopback address: 127.0.1.1; using 192.168.1.176 instead (on interface wlp1s0)\n",
      "22/12/06 14:39:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/graeme/anaconda3/envs/ENSF-612/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/06 14:39:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark;\n",
    "spark = SparkSession.builder.appName('sampler').getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_issues = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"multiline\", \"true\")\n",
    "  .option(\"quote\", '\"')  \n",
    "  .option(\"escape\", \"\\\\\")\n",
    "  .option(\"escape\", '\"')\n",
    "  .load(\"issues.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we prepare the dataframe for sampling. We made an initial filter with three parameters:\n",
    "\n",
    "1. We will use closed issues only.\n",
    "1. GitHub considers pull requests issues, so we have filtered out these as well.\n",
    "1. Only use issues after 2018. React was in it's infancy, and we noticed the issues back then were mostly internal and very different than more modern issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of issues: 24716\n",
      "Number of filter issues: 5655\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "closed_issues = all_issues.where((col(\"`pull_request.url`\").isNull())\n",
    "                                & (col(\"state\") == \"closed\")\n",
    "                                & (year(col(\"created_at\")) >= 2018)\n",
    "                                )\n",
    "\n",
    "print(\"Total number of issues:\", all_issues.count())\n",
    "print(\"Number of filter issues:\", closed_issues.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will select only the columns we think might be relevant to help us with our manual datapoint labeling (there are 111 cols in the original CSV).\n",
    "\n",
    "We will also add a new column that includes the year and month of the comment. We will stratify our random sample by year-month of 'creation_date' to try to get representative sample across the entire lifespan of the project. Some time periods may have had more activity due to recent major releases, and activity has picked up with the popularity of the project. We want to capture it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, year, month\n",
    "\n",
    "closed_issues = closed_issues.select(\"html_url\",\n",
    "        \"number\",\n",
    "        \"title\",\n",
    "        \"labels\",\n",
    "        \"state\",\n",
    "        \"locked\",\n",
    "        \"milestone\",\n",
    "        \"comments\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "        \"closed_at\",\n",
    "        \"author_association\",\n",
    "        \"state_reason\",\n",
    "        \"body\")\n",
    "closed_issues = closed_issues.withColumn(\"time_period\", concat(year(col(\"created_at\")),month(col(\"created_at\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sample of 1000 points from the closed_issues dataframe, stratified based on time_period to ensure we are getting a representative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# Determine the required fraction out of the total. We want 1000 samples, but had to choose 1059 due to rounding errors.\n",
    "fraction = 1059 / closed_issues.count()\n",
    "# Generate a fractions column for each distinct time_period in our df\n",
    "fractions = closed_issues.select(\"time_period\").distinct().withColumn(\"fraction\", lit(fraction)).rdd.collectAsMap()\n",
    "# Generate the samples stratified based on the \"time_period\"\n",
    "samples = closed_issues.stat.sampleBy(\"time_period\", fractions, seed=0)\n",
    "#ensure we have an adequete number of samples, after possible rounding errors\n",
    "samples.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column for each labeller to store their label in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.withColumn(\"Graeme\", lit(0)).withColumn(\"Steve\", lit(0)).withColumn(\"Dave\", lit(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the final list of samples to an excel file for manual labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "panda_df = samples.toPandas()\n",
    "panda_df.to_excel(\"ENSF 612 - Label Samples.xlsx\", index=False, engine=\"xlsxwriter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENSF-612",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "758f2f2736c7fa9cea9c3ba7e462cc50ad01011ab3fd1554d8a168a53b5df95b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
